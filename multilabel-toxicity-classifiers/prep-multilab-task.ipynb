{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a5a817-1412-4b78-8bde-a6c2f6bfed66",
   "metadata": {},
   "source": [
    "## Multilabel Task - Preprocess\n",
    "- import kaggle training/test set\n",
    "- delete all comments not usefull for scoring (= -1)\n",
    "- Text cleaning (emoji, non ASCII, email address, IP address,etc..)\n",
    "- Terms tokenization & reduction (no stopword, stemming, lemmatization, bigrams,..)\n",
    "- export trainin/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e665edd-fa3b-455c-87c1-a64f17c5cb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "753949bf-4b90-40c4-a11a-e7e92678c389",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 300\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1697a512-65d2-441f-90da-05835438230a",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a990a42-8ca5-4337-8ad8-933cbc6d795d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc9bbdd-572d-4b9b-8ce2-3566f2a902c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "test_lab = pd.read_csv(\"../data/test_labels.csv\")\n",
    "test = test.merge(test_lab, on=\"id\")\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c99584b-7482-403b-9a6f-14657cb95b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(312735, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([test.assign(ind=\"test\"), train.assign(ind=\"train\")])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3e16099-15ed-4d32-b701-21f084a3cbd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>\":::::And for the second time of asking, when your view completely contradicts the coverage in reliable sources, why should anyone care what you feel? You can't even give a consistent argument - is the opening only supposed to mention significant aspects, or the \"\"most significant\"\" ones?   \\n\\n\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is a horrible thing you put on my talk page.  128.61.19.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for prostitution ring.  - Crunch Captain.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>And it looks like it was actually you who put on the speedy to have the first version deleted now that I look at it.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand.  I came here and my idea was bad right away.  What kind of community goes \"\"you have bad ideas\"\" go away, instead of helping rewrite them.   \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id  \\\n",
       "159566  ffe987279560d7ff   \n",
       "159567  ffea4adeee384e90   \n",
       "159568  ffee36eab5c267c9   \n",
       "159569  fff125370e4aaaf3   \n",
       "159570  fff46fc426af1f9a   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     comment_text  \\\n",
       "159566  \":::::And for the second time of asking, when your view completely contradicts the coverage in reliable sources, why should anyone care what you feel? You can't even give a consistent argument - is the opening only supposed to mention significant aspects, or the \"\"most significant\"\" ones?   \\n\\n\"   \n",
       "159567                                                                                                                                                                                                      You should be ashamed of yourself \\n\\nThat is a horrible thing you put on my talk page.  128.61.19.93   \n",
       "159568                                                                                                                                                                                                                        Spitzer \\n\\nUmm, theres no actual article for prostitution ring.  - Crunch Captain.   \n",
       "159569                                                                                                                                                                                       And it looks like it was actually you who put on the speedy to have the first version deleted now that I look at it.   \n",
       "159570                                                                                                             \"\\nAnd ... I really don't think you understand.  I came here and my idea was bad right away.  What kind of community goes \"\"you have bad ideas\"\" go away, instead of helping rewrite them.   \"   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate    ind  \n",
       "159566      0             0        0       0       0              0  train  \n",
       "159567      0             0        0       0       0              0  train  \n",
       "159568      0             0        0       0       0              0  train  \n",
       "159569      0             0        0       0       0              0  train  \n",
       "159570      0             0        0       0       0              0  train  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da622bf1-3f78-4072-a137-441a741ba7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 312735 entries, 0 to 159570\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             312735 non-null  object\n",
      " 1   comment_text   312735 non-null  object\n",
      " 2   toxic          312735 non-null  int64 \n",
      " 3   severe_toxic   312735 non-null  int64 \n",
      " 4   obscene        312735 non-null  int64 \n",
      " 5   threat         312735 non-null  int64 \n",
      " 6   insult         312735 non-null  int64 \n",
      " 7   identity_hate  312735 non-null  int64 \n",
      " 8   ind            312735 non-null  object\n",
      "dtypes: int64(6), object(3)\n",
      "memory usage: 23.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22502978-3c66-4ad0-85b7-a56cfc2a2730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>312735.000000</td>\n",
       "      <td>312735.000000</td>\n",
       "      <td>312735.000000</td>\n",
       "      <td>312735.000000</td>\n",
       "      <td>312735.000000</td>\n",
       "      <td>312735.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.216803</td>\n",
       "      <td>-0.278907</td>\n",
       "      <td>-0.246362</td>\n",
       "      <td>-0.282978</td>\n",
       "      <td>-0.249035</td>\n",
       "      <td>-0.278411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.553674</td>\n",
       "      <td>0.462240</td>\n",
       "      <td>0.513134</td>\n",
       "      <td>0.455311</td>\n",
       "      <td>0.509224</td>\n",
       "      <td>0.463074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  312735.000000  312735.000000  312735.000000  312735.000000   \n",
       "mean       -0.216803      -0.278907      -0.246362      -0.282978   \n",
       "std         0.553674       0.462240       0.513134       0.455311   \n",
       "min        -1.000000      -1.000000      -1.000000      -1.000000   \n",
       "25%        -1.000000      -1.000000      -1.000000      -1.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate  \n",
       "count  312735.000000  312735.000000  \n",
       "mean       -0.249035      -0.278411  \n",
       "std         0.509224       0.463074  \n",
       "min        -1.000000      -1.000000  \n",
       "25%        -1.000000      -1.000000  \n",
       "50%         0.000000       0.000000  \n",
       "75%         0.000000       0.000000  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5e30225-c7c5-4c23-bef2-0fd584f72384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "comment_text     0\n",
       "toxic            0\n",
       "severe_toxic     0\n",
       "obscene          0\n",
       "threat           0\n",
       "insult           0\n",
       "identity_hate    0\n",
       "ind              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82ca1c42-1e27-4a11-b222-bd0380ba0ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all comments not used for scoring (value equals to -1)\n",
    "df = df[df['toxic']!= -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b63340a-ea8b-4ade-b6eb-588c133f7de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223549, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e7c9c7",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02e2e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20ccf98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.encode(\"ascii\", errors=\"ignore\").decode() #Remove non ASCII\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"(\\n|\\r|\\t)+\", \" \", text) #remove \\n, \\r, \\t\n",
    "    text = contractions.fix(text) #Remove contractions (i'm --> i am)\n",
    "    #text = spell(text) # Spelling is slow\n",
    "    text = re.sub(r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", \"\", text) #Remove IP Address\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text) #Remove URLs\n",
    "    text = re.sub(r\"(?:^|(?<=[^\\w@.)]))([\\w+-](\\.(?!\\.))?)*?[\\w+-]@(?:\\w-?)*?\\w+(\\.([a-z]{2,})){1,3}(?:$|(?=\\b))\", \"\", text) #Remove Email\n",
    "    text = re.sub(r\"\\[\\[User.*\", \"\", text) #Remove username\n",
    "    text = re.sub(r'[]!\"$%&\\'()*+,./:;=#@?[\\\\^_`{|}~-]+', \"\", text) #Remove punctuation (si vuole fare?)\n",
    "    #text = re.sub(r\"(\\w+.jpg$|\\w+.png$)\", \"\") #Remove filename (.jpg, .png, etc...)\n",
    "    text = text.rstrip() #remove additional white space\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64aa12c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223549/223549 [00:33<00:00, 6594.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "clean_texts=[]\n",
    "for text in tqdm.tqdm(df.comment_text):\n",
    "    clean_texts.append(clean_text(text))\n",
    "\n",
    "df['clean_text'] = clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c10fa13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\",\n",
       " 'explanation why the edits made under my username hardcore metallica fan were reverted they were not vandalisms just closure on some gas after i voted at new york dolls fac and please do not remove the template from the talk page since i am retired now')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comment_text'][0], df['clean_text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7197d6d5",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ead8316b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/fgemignani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/fgemignani/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/fgemignani/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/fgemignani/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/fgemignani/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "#PoS Tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\", \".join(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb6a6b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "df['tokenized'] = df['clean_text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db37d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize no stopwords\n",
    "df['tokenized_no_stopwords'] = df['tokenized'].apply(lambda words: [word for word in words if word not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9181f16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getrecursionlimit())\n",
    "sys.setrecursionlimit(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c03317b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223549/223549 [02:46<00:00, 1341.84it/s]\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "stem_list = []\n",
    "for row in tqdm.tqdm(df['tokenized_no_stopwords']):\n",
    "    stem_list.append([ stemmer.stem(word) for word in row ])\n",
    "\n",
    "df['tokenized_stemmed'] = stem_list\n",
    "#df['tokenized_stemmed'] = df['tokenized_no_stopwords'].apply(lambda words: [stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04c66146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223549/223549 [07:35<00:00, 490.83it/s]\n"
     ]
    }
   ],
   "source": [
    "#PoS\n",
    "pos_list = []\n",
    "for row in tqdm.tqdm(df['tokenized_no_stopwords']):\n",
    "    pos_list.append(nltk.pos_tag(row))\n",
    "\n",
    "df['tokenized_pos'] = pos_list\n",
    "#df['tokenized_pos'] = df['tokenized_no_stopwords'].apply(lambda words: nltk.pos_tag(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b64ba9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemma without PoS\n",
    "df['tokenized_lemmatized'] = df['tokenized_no_stopwords'].apply(lambda words: [lmtzr.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c915b5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223549/223549 [00:34<00:00, 6416.23it/s]\n"
     ]
    }
   ],
   "source": [
    "#Lemma with PoS\n",
    "lemma_list = []\n",
    "\n",
    "for words in tqdm.tqdm(df['tokenized_pos']):\n",
    "    tmp=[]\n",
    "    for lemma, pos in words:\n",
    "        if pos.startswith(\"NN\"):\n",
    "            tmp.append(lmtzr.lemmatize(lemma, pos='n'))\n",
    "        elif pos.startswith('VB'):\n",
    "            tmp.append(lmtzr.lemmatize(lemma, pos='v'))\n",
    "        elif pos.startswith('JJ'):\n",
    "            tmp.append(lmtzr.lemmatize(lemma, pos='a'))\n",
    "        elif pos.startswith('R'):\n",
    "            tmp.append(lmtzr.lemmatize(lemma, pos='r'))\n",
    "        else:\n",
    "            tmp.append(lmtzr.lemmatize(lemma))\n",
    "            \n",
    "    lemma_list.append(tmp)\n",
    "    \n",
    "df['tokenized_lemmatized_pos'] = lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84688384",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bigrams\n",
    "df['tokenized_bigram'] = df['tokenized_no_stopwords'].apply(lambda words: list(nltk.ngrams(words, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2aa70432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>ind</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tokenized_no_stopwords</th>\n",
       "      <th>tokenized_stemmed</th>\n",
       "      <th>tokenized_pos</th>\n",
       "      <th>tokenized_lemmatized</th>\n",
       "      <th>tokenized_lemmatized_pos</th>\n",
       "      <th>tokenized_bigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>Thank you for understanding. I think very highly of you and would not revert without discussion.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>thank you for understanding i think very highly of you and would not revert without discussion</td>\n",
       "      <td>[thank, you, for, understanding, i, think, very, highly, of, you, and, would, not, revert, without, discussion]</td>\n",
       "      <td>[thank, understanding, think, highly, would, revert, without, discussion]</td>\n",
       "      <td>[thank, understand, think, highli, would, revert, without, discuss]</td>\n",
       "      <td>[(thank, NN), (understanding, VBG), (think, VBP), (highly, RB), (would, MD), (revert, VB), (without, IN), (discussion, NN)]</td>\n",
       "      <td>[thank, understanding, think, highly, would, revert, without, discussion]</td>\n",
       "      <td>[thank, understand, think, highly, would, revert, without, discussion]</td>\n",
       "      <td>[(thank, understanding), (understanding, think), (think, highly), (highly, would), (would, revert), (revert, without), (without, discussion)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>dear god this site is horrible</td>\n",
       "      <td>[dear, god, this, site, is, horrible]</td>\n",
       "      <td>[dear, god, site, horrible]</td>\n",
       "      <td>[dear, god, site, horribl]</td>\n",
       "      <td>[(dear, JJ), (god, NN), (site, NN), (horrible, JJ)]</td>\n",
       "      <td>[dear, god, site, horrible]</td>\n",
       "      <td>[dear, god, site, horrible]</td>\n",
       "      <td>[(dear, god), (god, site), (site, horrible)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0002f87b16116a7f</td>\n",
       "      <td>\"::: Somebody will invariably try to add Religion?  Really??  You mean, the way people have invariably kept adding \"\"Religion\"\" to the Samuel Beckett infobox?  And why do you bother bringing up the long-dead completely non-existent \"\"Influences\"\" issue?  You're just flailing, making up crap on t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>somebody will invariably try to add religion  really  you mean the way people have invariably kept adding religion to the samuel beckett infobox  and why do you bother bringing up the longdead completely nonexistent influences issue  you are just flailing making up crap on the fly    for compar...</td>\n",
       "      <td>[somebody, will, invariably, try, to, add, religion, really, you, mean, the, way, people, have, invariably, kept, adding, religion, to, the, samuel, beckett, infobox, and, why, do, you, bother, bringing, up, the, longdead, completely, nonexistent, influences, issue, you, are, just, flailing, mak...</td>\n",
       "      <td>[somebody, invariably, try, add, religion, really, mean, way, people, invariably, kept, adding, religion, samuel, beckett, infobox, bother, bringing, longdead, completely, nonexistent, influences, issue, flailing, making, crap, fly, comparison, explicit, acknowledgement, entire, amos, oz, articl...</td>\n",
       "      <td>[somebodi, invari, tri, add, religion, realli, mean, way, peopl, invari, kept, ad, religion, samuel, beckett, infobox, bother, bring, longdead, complet, nonexist, influenc, issu, flail, make, crap, fli, comparison, explicit, acknowledg, entir, amo, oz, articl, person, jewish, categori]</td>\n",
       "      <td>[(somebody, NN), (invariably, RB), (try, VB), (add, RP), (religion, NN), (really, RB), (mean, JJ), (way, NN), (people, NNS), (invariably, RB), (kept, VBD), (adding, VBG), (religion, NN), (samuel, NN), (beckett, NN), (infobox, NN), (bother, IN), (bringing, VBG), (longdead, JJ), (completely, RB), ...</td>\n",
       "      <td>[somebody, invariably, try, add, religion, really, mean, way, people, invariably, kept, adding, religion, samuel, beckett, infobox, bother, bringing, longdead, completely, nonexistent, influence, issue, flailing, making, crap, fly, comparison, explicit, acknowledgement, entire, amos, oz, article...</td>\n",
       "      <td>[somebody, invariably, try, add, religion, really, mean, way, people, invariably, keep, add, religion, samuel, beckett, infobox, bother, bring, longdead, completely, nonexistent, influence, issue, flail, make, crap, fly, comparison, explicit, acknowledgement, entire, amos, oz, article, personall...</td>\n",
       "      <td>[(somebody, invariably), (invariably, try), (try, add), (add, religion), (religion, really), (really, mean), (mean, way), (way, people), (people, invariably), (invariably, kept), (kept, adding), (adding, religion), (religion, samuel), (samuel, beckett), (beckett, infobox), (infobox, bother), (bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0003e1cccfd5a40a</td>\n",
       "      <td>\" \\n\\n It says it right there that it IS a type. The \"\"Type\"\" of institution is needed in this case because there are three levels of SUNY schools: \\n -University Centers and Doctoral Granting Institutions \\n -State Colleges \\n -Community Colleges. \\n\\n It is needed in this case to clarify that ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>it says it right there that it is a type the type of institution is needed in this case because there are three levels of suny schools   university centers and doctoral granting institutions   state colleges   community colleges   it is needed in this case to clarify that ub is a suny center ...</td>\n",
       "      <td>[it, says, it, right, there, that, it, is, a, type, the, type, of, institution, is, needed, in, this, case, because, there, are, three, levels, of, suny, schools, university, centers, and, doctoral, granting, institutions, state, colleges, community, colleges, it, is, needed, in, this, case, to,...</td>\n",
       "      <td>[says, right, type, type, institution, needed, case, three, levels, suny, schools, university, centers, doctoral, granting, institutions, state, colleges, community, colleges, needed, case, clarify, ub, suny, center, says, even, binghamton, university, university, albany, state, university, new,...</td>\n",
       "      <td>[say, right, type, type, institut, need, case, three, level, suni, school, univers, center, doctor, grant, institut, state, colleg, commun, colleg, need, case, clarifi, ub, suni, center, say, even, binghamton, univers, univers, albani, state, univers, new, york, stoni, brook, univers, stop, tri,...</td>\n",
       "      <td>[(says, VBZ), (right, JJ), (type, NN), (type, NN), (institution, NN), (needed, VBD), (case, NN), (three, CD), (levels, NNS), (suny, VBP), (schools, NNS), (university, NN), (centers, NNS), (doctoral, JJ), (granting, VBG), (institutions, NNS), (state, NN), (colleges, NNS), (community, NN), (colleg...</td>\n",
       "      <td>[say, right, type, type, institution, needed, case, three, level, suny, school, university, center, doctoral, granting, institution, state, college, community, college, needed, case, clarify, ub, suny, center, say, even, binghamton, university, university, albany, state, university, new, york, s...</td>\n",
       "      <td>[say, right, type, type, institution, need, case, three, level, suny, school, university, center, doctoral, grant, institution, state, college, community, college, need, case, clarify, ub, suny, center, say, even, binghamton, university, university, albany, state, university, new, york, stony, b...</td>\n",
       "      <td>[(says, right), (right, type), (type, type), (type, institution), (institution, needed), (needed, case), (case, three), (three, levels), (levels, suny), (suny, schools), (schools, university), (university, centers), (centers, doctoral), (doctoral, granting), (granting, institutions), (institutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00059ace3e3e9a53</td>\n",
       "      <td>\" \\n\\n == Before adding a new product to the list, make sure it's relevant == \\n\\n Before adding a new product to the list, make sure it has a wikipedia entry already, \"\"proving\"\" it's relevance and giving the reader the possibility to read more about it. \\n Otherwise it could be subject to dele...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>before adding a new product to the list make sure it is relevant    before adding a new product to the list make sure it has a wikipedia entry already proving it is relevance and giving the reader the possibility to read more about it   otherwise it could be subject to deletion see this arti...</td>\n",
       "      <td>[before, adding, a, new, product, to, the, list, make, sure, it, is, relevant, before, adding, a, new, product, to, the, list, make, sure, it, has, a, wikipedia, entry, already, proving, it, is, relevance, and, giving, the, reader, the, possibility, to, read, more, about, it, otherwise, it, coul...</td>\n",
       "      <td>[adding, new, product, list, make, sure, relevant, adding, new, product, list, make, sure, wikipedia, entry, already, proving, relevance, giving, reader, possibility, read, otherwise, could, subject, deletion, see, articles, revision, history]</td>\n",
       "      <td>[ad, new, product, list, make, sure, relev, ad, new, product, list, make, sure, wikipedia, entri, alreadi, prove, relev, give, reader, possibl, read, otherwis, could, subject, delet, see, articl, revis, histori]</td>\n",
       "      <td>[(adding, VBG), (new, JJ), (product, NN), (list, NN), (make, VBP), (sure, JJ), (relevant, JJ), (adding, VBG), (new, JJ), (product, NN), (list, NN), (make, VBP), (sure, JJ), (wikipedia, JJ), (entry, NN), (already, RB), (proving, VBG), (relevance, NN), (giving, VBG), (reader, NN), (possibility, NN...</td>\n",
       "      <td>[adding, new, product, list, make, sure, relevant, adding, new, product, list, make, sure, wikipedia, entry, already, proving, relevance, giving, reader, possibility, read, otherwise, could, subject, deletion, see, article, revision, history]</td>\n",
       "      <td>[add, new, product, list, make, sure, relevant, add, new, product, list, make, sure, wikipedia, entry, already, prove, relevance, give, reader, possibility, read, otherwise, could, subject, deletion, see, article, revision, history]</td>\n",
       "      <td>[(adding, new), (new, product), (product, list), (list, make), (make, sure), (sure, relevant), (relevant, adding), (adding, new), (new, product), (product, list), (list, make), (make, sure), (sure, wikipedia), (wikipedia, entry), (entry, already), (already, proving), (proving, relevance), (relev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id  \\\n",
       "5   0001ea8717f6de06   \n",
       "7   000247e83dcc1211   \n",
       "11  0002f87b16116a7f   \n",
       "13  0003e1cccfd5a40a   \n",
       "14  00059ace3e3e9a53   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                   comment_text  \\\n",
       "5                                                                                                                                                                                                              Thank you for understanding. I think very highly of you and would not revert without discussion.   \n",
       "7                                                                                                                                                                                                                                                                              :Dear god this site is horrible.   \n",
       "11  \"::: Somebody will invariably try to add Religion?  Really??  You mean, the way people have invariably kept adding \"\"Religion\"\" to the Samuel Beckett infobox?  And why do you bother bringing up the long-dead completely non-existent \"\"Influences\"\" issue?  You're just flailing, making up crap on t...   \n",
       "13  \" \\n\\n It says it right there that it IS a type. The \"\"Type\"\" of institution is needed in this case because there are three levels of SUNY schools: \\n -University Centers and Doctoral Granting Institutions \\n -State Colleges \\n -Community Colleges. \\n\\n It is needed in this case to clarify that ...   \n",
       "14  \" \\n\\n == Before adding a new product to the list, make sure it's relevant == \\n\\n Before adding a new product to the list, make sure it has a wikipedia entry already, \"\"proving\"\" it's relevance and giving the reader the possibility to read more about it. \\n Otherwise it could be subject to dele...   \n",
       "\n",
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate   ind  \\\n",
       "5       0             0        0       0       0              0  test   \n",
       "7       0             0        0       0       0              0  test   \n",
       "11      0             0        0       0       0              0  test   \n",
       "13      0             0        0       0       0              0  test   \n",
       "14      0             0        0       0       0              0  test   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                     clean_text  \\\n",
       "5                                                                                                                                                                                                                thank you for understanding i think very highly of you and would not revert without discussion   \n",
       "7                                                                                                                                                                                                                                                                                dear god this site is horrible   \n",
       "11   somebody will invariably try to add religion  really  you mean the way people have invariably kept adding religion to the samuel beckett infobox  and why do you bother bringing up the longdead completely nonexistent influences issue  you are just flailing making up crap on the fly    for compar...   \n",
       "13     it says it right there that it is a type the type of institution is needed in this case because there are three levels of suny schools   university centers and doctoral granting institutions   state colleges   community colleges   it is needed in this case to clarify that ub is a suny center ...   \n",
       "14      before adding a new product to the list make sure it is relevant    before adding a new product to the list make sure it has a wikipedia entry already proving it is relevance and giving the reader the possibility to read more about it   otherwise it could be subject to deletion see this arti...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      tokenized  \\\n",
       "5                                                                                                                                                                                               [thank, you, for, understanding, i, think, very, highly, of, you, and, would, not, revert, without, discussion]   \n",
       "7                                                                                                                                                                                                                                                                         [dear, god, this, site, is, horrible]   \n",
       "11  [somebody, will, invariably, try, to, add, religion, really, you, mean, the, way, people, have, invariably, kept, adding, religion, to, the, samuel, beckett, infobox, and, why, do, you, bother, bringing, up, the, longdead, completely, nonexistent, influences, issue, you, are, just, flailing, mak...   \n",
       "13  [it, says, it, right, there, that, it, is, a, type, the, type, of, institution, is, needed, in, this, case, because, there, are, three, levels, of, suny, schools, university, centers, and, doctoral, granting, institutions, state, colleges, community, colleges, it, is, needed, in, this, case, to,...   \n",
       "14  [before, adding, a, new, product, to, the, list, make, sure, it, is, relevant, before, adding, a, new, product, to, the, list, make, sure, it, has, a, wikipedia, entry, already, proving, it, is, relevance, and, giving, the, reader, the, possibility, to, read, more, about, it, otherwise, it, coul...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                         tokenized_no_stopwords  \\\n",
       "5                                                                                                                                                                                                                                     [thank, understanding, think, highly, would, revert, without, discussion]   \n",
       "7                                                                                                                                                                                                                                                                                   [dear, god, site, horrible]   \n",
       "11  [somebody, invariably, try, add, religion, really, mean, way, people, invariably, kept, adding, religion, samuel, beckett, infobox, bother, bringing, longdead, completely, nonexistent, influences, issue, flailing, making, crap, fly, comparison, explicit, acknowledgement, entire, amos, oz, articl...   \n",
       "13  [says, right, type, type, institution, needed, case, three, levels, suny, schools, university, centers, doctoral, granting, institutions, state, colleges, community, colleges, needed, case, clarify, ub, suny, center, says, even, binghamton, university, university, albany, state, university, new,...   \n",
       "14                                                          [adding, new, product, list, make, sure, relevant, adding, new, product, list, make, sure, wikipedia, entry, already, proving, relevance, giving, reader, possibility, read, otherwise, could, subject, deletion, see, articles, revision, history]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                              tokenized_stemmed  \\\n",
       "5                                                                                                                                                                                                                                           [thank, understand, think, highli, would, revert, without, discuss]   \n",
       "7                                                                                                                                                                                                                                                                                    [dear, god, site, horribl]   \n",
       "11               [somebodi, invari, tri, add, religion, realli, mean, way, peopl, invari, kept, ad, religion, samuel, beckett, infobox, bother, bring, longdead, complet, nonexist, influenc, issu, flail, make, crap, fli, comparison, explicit, acknowledg, entir, amo, oz, articl, person, jewish, categori]   \n",
       "13  [say, right, type, type, institut, need, case, three, level, suni, school, univers, center, doctor, grant, institut, state, colleg, commun, colleg, need, case, clarifi, ub, suni, center, say, even, binghamton, univers, univers, albani, state, univers, new, york, stoni, brook, univers, stop, tri,...   \n",
       "14                                                                                          [ad, new, product, list, make, sure, relev, ad, new, product, list, make, sure, wikipedia, entri, alreadi, prove, relev, give, reader, possibl, read, otherwis, could, subject, delet, see, articl, revis, histori]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                  tokenized_pos  \\\n",
       "5                                                                                                                                                                                   [(thank, NN), (understanding, VBG), (think, VBP), (highly, RB), (would, MD), (revert, VB), (without, IN), (discussion, NN)]   \n",
       "7                                                                                                                                                                                                                                                           [(dear, JJ), (god, NN), (site, NN), (horrible, JJ)]   \n",
       "11  [(somebody, NN), (invariably, RB), (try, VB), (add, RP), (religion, NN), (really, RB), (mean, JJ), (way, NN), (people, NNS), (invariably, RB), (kept, VBD), (adding, VBG), (religion, NN), (samuel, NN), (beckett, NN), (infobox, NN), (bother, IN), (bringing, VBG), (longdead, JJ), (completely, RB), ...   \n",
       "13  [(says, VBZ), (right, JJ), (type, NN), (type, NN), (institution, NN), (needed, VBD), (case, NN), (three, CD), (levels, NNS), (suny, VBP), (schools, NNS), (university, NN), (centers, NNS), (doctoral, JJ), (granting, VBG), (institutions, NNS), (state, NN), (colleges, NNS), (community, NN), (colleg...   \n",
       "14  [(adding, VBG), (new, JJ), (product, NN), (list, NN), (make, VBP), (sure, JJ), (relevant, JJ), (adding, VBG), (new, JJ), (product, NN), (list, NN), (make, VBP), (sure, JJ), (wikipedia, JJ), (entry, NN), (already, RB), (proving, VBG), (relevance, NN), (giving, VBG), (reader, NN), (possibility, NN...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                           tokenized_lemmatized  \\\n",
       "5                                                                                                                                                                                                                                     [thank, understanding, think, highly, would, revert, without, discussion]   \n",
       "7                                                                                                                                                                                                                                                                                   [dear, god, site, horrible]   \n",
       "11  [somebody, invariably, try, add, religion, really, mean, way, people, invariably, kept, adding, religion, samuel, beckett, infobox, bother, bringing, longdead, completely, nonexistent, influence, issue, flailing, making, crap, fly, comparison, explicit, acknowledgement, entire, amos, oz, article...   \n",
       "13  [say, right, type, type, institution, needed, case, three, level, suny, school, university, center, doctoral, granting, institution, state, college, community, college, needed, case, clarify, ub, suny, center, say, even, binghamton, university, university, albany, state, university, new, york, s...   \n",
       "14                                                           [adding, new, product, list, make, sure, relevant, adding, new, product, list, make, sure, wikipedia, entry, already, proving, relevance, giving, reader, possibility, read, otherwise, could, subject, deletion, see, article, revision, history]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                       tokenized_lemmatized_pos  \\\n",
       "5                                                                                                                                                                                                                                        [thank, understand, think, highly, would, revert, without, discussion]   \n",
       "7                                                                                                                                                                                                                                                                                   [dear, god, site, horrible]   \n",
       "11  [somebody, invariably, try, add, religion, really, mean, way, people, invariably, keep, add, religion, samuel, beckett, infobox, bother, bring, longdead, completely, nonexistent, influence, issue, flail, make, crap, fly, comparison, explicit, acknowledgement, entire, amos, oz, article, personall...   \n",
       "13  [say, right, type, type, institution, need, case, three, level, suny, school, university, center, doctoral, grant, institution, state, college, community, college, need, case, clarify, ub, suny, center, say, even, binghamton, university, university, albany, state, university, new, york, stony, b...   \n",
       "14                                                                     [add, new, product, list, make, sure, relevant, add, new, product, list, make, sure, wikipedia, entry, already, prove, relevance, give, reader, possibility, read, otherwise, could, subject, deletion, see, article, revision, history]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                               tokenized_bigram  \n",
       "5                                                                                                                                                                 [(thank, understanding), (understanding, think), (think, highly), (highly, would), (would, revert), (revert, without), (without, discussion)]  \n",
       "7                                                                                                                                                                                                                                                                  [(dear, god), (god, site), (site, horrible)]  \n",
       "11  [(somebody, invariably), (invariably, try), (try, add), (add, religion), (religion, really), (really, mean), (mean, way), (way, people), (people, invariably), (invariably, kept), (kept, adding), (adding, religion), (religion, samuel), (samuel, beckett), (beckett, infobox), (infobox, bother), (bo...  \n",
       "13  [(says, right), (right, type), (type, type), (type, institution), (institution, needed), (needed, case), (case, three), (three, levels), (levels, suny), (suny, schools), (schools, university), (university, centers), (centers, doctoral), (doctoral, granting), (granting, institutions), (institutio...  \n",
       "14  [(adding, new), (new, product), (product, list), (list, make), (make, sure), (sure, relevant), (relevant, adding), (adding, new), (new, product), (product, list), (list, make), (make, sure), (sure, wikipedia), (wikipedia, entry), (entry, already), (already, proving), (proving, relevance), (relev...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6fbf40-fe26-48c7-bcf5-ae5376d8dff1",
   "metadata": {},
   "source": [
    "### Export training/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20525288-335f-451f-b6ca-0e737103252c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data:\n",
      "train    159571\n",
      "test      63978\n",
      "Name: ind, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"All data:\\n{df['ind'].value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f40d68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df[df['ind']=='train']\n",
    "df_train.drop('ind',axis=1,inplace=True)\n",
    "df_train.to_csv('./train_cleaned.zip',index=False)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be4dd68b-4d45-4859-974b-0bbedb6532dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63978, 16)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df[df['ind']=='test']\n",
    "df_test.drop('ind',axis=1,inplace=True)\n",
    "df_test.to_csv('./test_cleaned.zip',index=False)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b29d30a-5d6c-455d-b22d-e986e30086c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
